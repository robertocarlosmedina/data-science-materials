{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">\n",
    " <div class=\"alert alert-block alert-info\"><b>Master in Data Science - Iscte <b>\n",
    "     </div>\n",
    "</font> \n",
    " \n",
    " \n",
    "     \n",
    "    \n",
    "  <font size=\"5\"> OEOD </font>\n",
    "  \n",
    "  \n",
    "  \n",
    "  <font size=\"3\"> **Diana Aldea Mendes**, November 2023 </font>\n",
    "  \n",
    "   \n",
    "  <font size=\"3\"> *diana.mendes@iscte-iul.pt* </font> \n",
    "  \n",
    "    \n",
    " \n",
    "  \n",
    "    \n",
    "  <font color='blue'><font size=\"5\"> <b>Week 8 - Case study 3 - RL for Healthcare <b></font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt7GkVURKLVn"
   },
   "source": [
    "# RL for Healthcare\n",
    "\n",
    "\n",
    "- Reinforcement Learning applications in healthcare\n",
    "    - Drug dosage (Q-learning, deep Q-learning) (for example insulin for diabetes)\n",
    "    - Dynamic Treatment Recommendation (chronic diseases and critical care) (actor-critic with RNN)\n",
    "    - Medical Diagnosis\n",
    "    - Resource scheduling and Allocation\n",
    "    - Drug discovery\n",
    "    - Health Management\n",
    "\n",
    "- Take a look here: https://healthgym.ai/\n",
    "- https://vadim.me/publications/heartpole/\n",
    "\n",
    "\n",
    "- Reinforcement learning for healthcare applications:\n",
    "\n",
    "1. Define the Environment:\n",
    "   - Identify the healthcare problem you want to address, such as treatment plan optimization or clinical decision-making.\n",
    "   - Define the state representation: Determine the relevant patient information, medical history, diagnostic test results, and other relevant factors that will constitute the state space.\n",
    "   - Define the action space: Determine the set of possible actions, such as treatment options, medication dosages, or diagnostic tests to be ordered.\n",
    "   - Define the reward function: Design a reward function that reflects the effectiveness or quality of the treatment plan or decision made. For example, the reward could be based on patient health outcomes, cost-effectiveness, or adherence to medical guidelines.\n",
    "\n",
    "2. Implement the Reinforcement Learning Algorithm:\n",
    "   - Choose a reinforcement learning algorithm suitable for your healthcare problem, such as Q-learning, deep Q-networks (DQN), or actor-critic methods.\n",
    "   - Initialize the necessary components, such as the Q-table or the neural network architecture.\n",
    "   - Implement the training loop:\n",
    "     - Sample a state from the environment.\n",
    "     - Select an action using an exploration-exploitation strategy, such as epsilon-greedy or softmax.\n",
    "     - Execute the chosen action and observe the next state and the reward.\n",
    "     - Update the Q-values or adjust the model weights based on the chosen algorithm.\n",
    "     - Repeat the above steps until convergence or a sufficient number of iterations.\n",
    "\n",
    "3. Data Collection and Preprocessing:\n",
    "   - Collect and preprocess the healthcare data required for training and evaluation. This may include patient records, medical imaging data, laboratory results, or clinical guidelines.\n",
    "   - Apply necessary data transformations and feature engineering techniques to prepare the data for input to the reinforcement learning algorithm.\n",
    "\n",
    "4. Training and Evaluation:\n",
    "   - Split the data into training and evaluation sets.\n",
    "   - Train the reinforcement learning model on the training data using the implemented algorithm.\n",
    "   - Evaluate the trained model's performance on the evaluation data, measuring relevant metrics such as patient outcomes, cost-effectiveness, or guideline adherence.\n",
    "\n",
    "5. Iterative Improvement:\n",
    "   - Analyze the results and iteratively refine the reinforcement learning model or the environment based on the observed performance.\n",
    "   - Adjust hyperparameters, modify the reward function, or consider incorporating additional data sources to enhance the model's effectiveness and generalizability.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Hospital Warehouse\n",
    "\n",
    "- **Goal**: develop a model that uses RL for the case of a hospital warehouse.\n",
    "\n",
    "## Model Description:\n",
    "\n",
    "- **Agent**: \n",
    "    - A single hospital warehouse subject to a stochastic customer demand (such as, for example, a Uniform distribution between values a and b).\n",
    "    - The warehouse is the **decision making agent**, deciding if it will either order additional supplies or if it will deliver supplies to customers.\n",
    "- **Reward**:\n",
    "    - The Goal is to maximize a reward, which for a hospital is the internal revenue obtained from a delivery, this is, income minus expenses.\n",
    "\n",
    "- The Reward experienced by this Warehouse is the total revenue, which consists of\n",
    "    - a. Sales Price of the items sold\n",
    "    - b. MINUS the holding cost of the Inventory\n",
    "    - c. MINUS the penality for Customer Demand not met\n",
    "- **Actions**\n",
    "    - The warehouse holds inventory (no maximum, and held in whole quantities) and can take two actions:\n",
    "        - a. Sell inventory to a customer\n",
    "        - b. Receive inventory from its supplier\n",
    "    - The warehouse can only do one of this actions at a time.\n",
    "- **State**: The State of the model is the warehouse inventory level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:06.556578Z",
     "start_time": "2023-11-19T21:32:06.547418Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np      \n",
    "import math\n",
    "import time             # calculate execution time\n",
    "import pandas as pd     # manipulate Dataframes \n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)  # number of digits for float type and do not write 1e-n\n",
    "\n",
    "# high-quality figures\n",
    "%config InlineBackend.figure_format = 'svg'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________\n",
    "\n",
    "- **Model parameters**:\n",
    "    - The model has cost and price parameters defined at the beginning of the model (do not substantially change when the model is executed).\n",
    "    - The Warehouse has as many *states* as the number of possible units in its stock\n",
    "    - The number of units (contained by the warehouse) is between 0 and **max_state** (that is, the Warehouse *size*). \n",
    "    - The initial state (initial warehouse stock) will be a random number between 0 and max_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:07.870832Z",
     "start_time": "2023-11-19T21:32:07.866243Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_inventory = 2     # The cost of holding inventory\n",
    "purchase_price = 20    # The price at which the inventory is bought\n",
    "sales_price = 50       # The price at which the inventory is sold\n",
    "\n",
    "max_state = 800\n",
    "initial_state = np.random.randint(max_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________\n",
    "\n",
    "- *Reward structure matrix* (**R matrix**) is a square matrix of size (max_state * max_state). \n",
    "- It contains the rewards and therefore also defines the possible actions that can be taken. \n",
    "- First we initialize the R Matrix, and after we fill the reward values \n",
    "- The X and Y coordinates of the Matrix correspond to the initial and final values of the decision.\n",
    "- For example, the R Matrix value at [10, 45] represents that the state is moving from 10 to 45, this means that 35 stock items have been added to the warehouse. \n",
    "- Therefore, the reward is negative and correspond to the cost of purchasing 35 new stock items + the cost of inventory at the beginning of the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:10.778829Z",
     "start_time": "2023-11-19T21:32:09.201698Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize R-matrix (all entries are zero)\n",
    "\n",
    "R = np.matrix(np.zeros([max_state,max_state]),dtype=int)\n",
    "\n",
    "# define the rule to fill the reward values in the R matrix\n",
    "\n",
    "for y in range(0, max_state):\n",
    "    for x in range(0, max_state):\n",
    "        R[x,y] = np.maximum((x-y)*sales_price,0)-np.maximum((y-x)*purchase_price,0)-x*cost_inventory\n",
    "        \n",
    "print('R: \\n', R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________\n",
    "\n",
    "- The **Q matrix** capture the total future reward for an agent from a given state after a certain action\n",
    "- The Q matrix has the same dimensions as the R Matrix. \n",
    "- It is initialized with random numbers entries. \n",
    "- The *learning rate* and the *discount rate* are also required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:12.139672Z",
     "start_time": "2023-11-19T21:32:12.129117Z"
    }
   },
   "outputs": [],
   "source": [
    "## initialize Q matrix with random number entries\n",
    "\n",
    "Q = np.matrix(np.random.random([max_state,max_state]))\n",
    "\n",
    "# define hyperparameters\n",
    "\n",
    "learning_rate = 0.5\n",
    "discount = 0.7\n",
    "EPISODES =1000\n",
    "STEPS = 200\n",
    "PRINT_EVERY = EPISODES/50\n",
    "Pepsilon_init = 0.8   # initial value for the decayed-epsilon-greedy method\n",
    "Pepsilon_end = 0.1    # final value for the decayed-epsilon-greedy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________\n",
    "\n",
    "- Now we define the **functions** we are going to use in the algorithm. \n",
    "- First, we consider a function that determine the **available actions** from which we can choose the next action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:13.345370Z",
     "start_time": "2023-11-19T21:32:13.338366Z"
    }
   },
   "outputs": [],
   "source": [
    "## function that determine the set of possible actions for the next action\n",
    "\n",
    "def available_actions(state, customers):\n",
    "    # The available actions are \n",
    "    #    a.- Meeting a customer requirement (going to s: state-order)\n",
    "    #    b.- Buying Inventory from Supplier (going to s: state+purchase)\n",
    "    purchase = np.arange(state, max_state) # Calculate all possible future states due to purchases from the current state\n",
    "    # print('Purchase: ',purchase)\n",
    "    new_customers_state =[]\n",
    "    new_customers_state = [np.maximum(state-x,0) for x in customers] # calculate the possible states from customers in the current state\n",
    "    # print('new_customers_state: ', new_customers_state)\n",
    "    return np.concatenate((purchase,new_customers_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________\n",
    "\n",
    "- Next, we define a function that randomly chooses which action to be performed within the range of available actions (that is, the future action). \n",
    "- There is a range of options given by:\n",
    "    1. Always choose the action that has the **maximum Q value** for the current_state (*exploit*)\n",
    "    2. Always choose a random action from the available actions for the current_state (*explore*)\n",
    "    3. Alternate between exploitation and exploration with a certain probability (which may change overtime)\n",
    "- The general recommendation is to start exploring most of the time and gradually decrease exploration to maximize exploitation.\n",
    "- For this we use the decayed-epsilon-greedy method where epsilon is the probability that a random action is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:14.727220Z",
     "start_time": "2023-11-19T21:32:14.721568Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_next_action(available_act, epsilon):\n",
    "    # choose the type of the next action to take. \n",
    "    # 1 for a random next action with probability epsilon \n",
    "    # 0 for a greedy next action with probability (1-epsilon)\n",
    "    random_action = np.random.binomial(n=1, p=epsilon, size=1)\n",
    "    \n",
    "    if random_action == 1:\n",
    "        # This is the option for full exploration - always random\n",
    "        # print('random action')\n",
    "        next_action = int(np.random.choice(available_act, 1))\n",
    "    else:\n",
    "        # This is the option for full exploitation - always use what we know (Greedy method)\n",
    "        # Choose the next actions from the available actions, and the one with the highest Q Value\n",
    "        # print('greedy action')\n",
    "        next_action = int(np.where(Q[current_state,] == np.max(Q[current_state,available_act]))[1])\n",
    "    \n",
    "    # now calculate the amount that is being sold or purchsed, if at all\n",
    "    if next_action < current_state:\n",
    "        Qsale = current_state-next_action\n",
    "        Qpurchase = 0\n",
    "    else: \n",
    "        Qpurchase = next_action - current_state\n",
    "        Qsale = 0\n",
    "    return next_action, Qsale, Qpurchase\n",
    "    \n",
    "def cost_inventory_backlog(current_state):\n",
    "    if current_state<=0:\n",
    "        return cost_backlog\n",
    "    else:\n",
    "        return cost_inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________\n",
    "\n",
    "\n",
    "- The following function updates the **Q table** with a formula (Bellman Q-equation) that requires the parameters:\n",
    "    - Q[current_state, action] = value to update (the action is the future state value the agent decided to take).\n",
    "    - learning_rate = value between 0 and 1 indicating how much new information over ride sold information.\n",
    "    - R[current_state, action] = Reward obtained when transitioning from current_state to future_state \n",
    "    - Q[future_state, a] = the max Q value of the possible actions a in the future_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:16.577712Z",
     "start_time": "2023-11-19T21:32:16.571143Z"
    }
   },
   "outputs": [],
   "source": [
    "# this function updates the Q matrix/table according to the path selected and the Q learning algorithm\n",
    "\n",
    "def update(current_state, action):\n",
    "   \n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[1] # index for the maximum Q value in the future_state\n",
    "    # print('Q[action,]: \\n', Q[action,])\n",
    "    # print('Current State: ', current_state)\n",
    "    # print('Action: ', action)\n",
    "    # print('Max Index:', max_index)\n",
    "    \n",
    "    # just in case there are more than one maxima, in which case we choose one randomly\n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, size=1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    max_value = Q[action, max_index] # this is the maximum Q value in the future state given the action that generates that maximum value\n",
    "    \n",
    "    # Q learning formula\n",
    "    Q[current_state, action] = (1-learning_rate)*Q[current_state, action] + learning_rate*(R[current_state, action] + discount*max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:33.592640Z",
     "start_time": "2023-11-19T21:32:17.914051Z"
    }
   },
   "outputs": [],
   "source": [
    "### training the simulation\n",
    "# Training\n",
    "\n",
    "start_time = time.time()\n",
    "epsilon = Pepsilon_init\n",
    "epsilon_delta = (Pepsilon_init - Pepsilon_end)/EPISODES\n",
    "\n",
    "calculation_times = []\n",
    "total_reward = []\n",
    "total_demand =[]\n",
    "\n",
    "total_jump = []\n",
    "jump_max = []\n",
    "jump_min = []\n",
    "jump_av =[]\n",
    "jump_sd = []\n",
    "\n",
    "total_state= []\n",
    "state_max = []\n",
    "state_min = []\n",
    "state_av =[]\n",
    "state_sd = []\n",
    "\n",
    "current_state = 0\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # Initialize values for Step\n",
    "    total_reward_episode = 0\n",
    "    start_time_episode = time.time()\n",
    "    \n",
    "    state_episode = []\n",
    "    jump_episode = []\n",
    "\n",
    "    # determine if this episode is generating a status output\n",
    "    if episode%PRINT_EVERY == 0:\n",
    "        print('Calc. Episode {} of {}, {:.1f}% progress'.format(episode, EPISODES, 100*(episode/EPISODES)))\n",
    "    # Execute the steps in the Episode    \n",
    "    for step in range(STEPS):\n",
    "        # Create a customer for this step\n",
    "        customers = []\n",
    "        customers.append(np.random.randint(0,max_state))\n",
    "        # Calculate the actions (future states) that are available from current state\n",
    "        available_act = available_actions(current_state, customers)\n",
    "        # Choose an action from the available future states\n",
    "        action = sample_next_action(available_act, epsilon)\n",
    "        # Update the Q table \n",
    "        update(current_state, action[0])\n",
    "\n",
    "        # record the states for the step\n",
    "        \n",
    "        total_state.append(current_state)\n",
    "        state_episode.append(current_state)\n",
    "        total_demand.append(customers[0])\n",
    "        total_reward_episode += R[current_state, action[0]]\n",
    "        total_jump.append(action[0] - current_state)\n",
    "        jump_episode.append(action[0] - current_state)\n",
    "        # update the state for the next step\n",
    "        current_state = action[0]\n",
    "\n",
    "    # record the states for the Episode\n",
    "    total_reward.append(total_reward_episode) # Total reward for the episode\n",
    "    calculation_times.append(time.time()-start_time_episode)\n",
    "    \n",
    "    jump_max.append(np.max(jump_episode))\n",
    "    jump_min.append(np.min(jump_episode))\n",
    "    jump_av.append(np.mean(jump_episode))\n",
    "    jump_sd.append(np.std(jump_episode))\n",
    "    \n",
    "    state_max.append(np.max(state_episode))\n",
    "    state_min.append(np.min(state_episode))\n",
    "    state_av.append(np.mean(state_episode))\n",
    "    state_sd.append(np.std(state_episode))\n",
    "\n",
    "    # Update parameters for the next episode\n",
    "    epsilon = Pepsilon_init - episode*epsilon_delta\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "\n",
    "# print out the total calculation time\n",
    "print('total calculation time: {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:39.215025Z",
     "start_time": "2023-11-19T21:32:37.269861Z"
    }
   },
   "outputs": [],
   "source": [
    "## plot results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "%matplotlib inline\n",
    "\n",
    "## subplot with (2x3)=6 figures\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(8, 6))\n",
    "\n",
    "MA_total_reward = pd.DataFrame(total_reward)\n",
    "\n",
    "Rolling_total_reward = MA_total_reward.rolling(window=5).mean()\n",
    "\n",
    "## labels and other info for each one of the 6 subplots\n",
    "\n",
    "axs[0,0].plot(MA_total_reward, label='Episode')\n",
    "axs[0,0].plot(Rolling_total_reward, color='r', label ='MA(5)')\n",
    "axs[0,0].set_title('Total Rewards')\n",
    "axs[0,0].set_ylabel('Rewards [$]')\n",
    "axs[0,0].set_xlabel('Episodes')\n",
    "axs[0,0].grid(axis='y', alpha=0.75)\n",
    "axs[0,0].grid(axis='x', alpha=0.75)\n",
    "\n",
    "axs[1,0].plot(calculation_times)\n",
    "axs[1,0].set_title('Calc. Times')\n",
    "axs[1,0].set_xlabel('Episodes')\n",
    "axs[1,0].set_ylabel('Calculation times [s]')\n",
    "axs[1,0].grid(axis='y', alpha=0.75)\n",
    "axs[1,0].grid(axis='x', alpha=0.75)\n",
    "\n",
    "axs[0,1].hist(total_state,color='#0504aa',alpha=0.7, rwidth=0.85)\n",
    "axs[0,1].set_title('States Histogram')\n",
    "axs[0,1].set_xlabel('State')\n",
    "axs[0,1].set_ylabel('Frequency')\n",
    "axs[0,1].set_xlim(xmin=0, xmax=max_state)\n",
    "axs[0,1].grid(axis='y', alpha=0.75)\n",
    "\n",
    "axs[1,1].plot(jump_max,color='b', label = 'max')\n",
    "axs[1,1].plot(jump_min,color='r', label = 'min')\n",
    "axs[1,1].plot(jump_av,color='g', label = 'av')\n",
    "axs[1,1].plot(jump_sd,color='y', label = 'sd')\n",
    "axs[1,1].set_title('Jumps')\n",
    "axs[1,1].legend()\n",
    "axs[1,1].set_xlabel('Episode')\n",
    "axs[1,1].set_ylabel('Jump Value')\n",
    "axs[1,1].grid(axis='y', alpha=0.75)\n",
    "\n",
    "axs[0,2].hist(total_jump,color='#0504aa',alpha=0.7, rwidth=0.85)\n",
    "axs[0,2].set_title('Jump Histogram')\n",
    "axs[0,2].set_xlabel('New_State-Old_State')\n",
    "axs[0,2].set_ylabel('Frequency')\n",
    "axs[0,2].set_xlim(xmin=-max_state, xmax=max_state)\n",
    "axs[0,2].grid(axis='y', alpha=0.75)\n",
    "\n",
    "axs[1,2].plot(state_max,color='b', label = 'max')\n",
    "axs[1,2].plot(state_min,color='r', label = 'min')\n",
    "axs[1,2].plot(state_av,color='g', label = 'av')\n",
    "axs[1,2].plot(state_sd,color='y', label = 'sd')\n",
    "axs[1,2].set_title('States')\n",
    "axs[1,2].legend()\n",
    "axs[1,2].set_xlabel('Episode')\n",
    "axs[1,2].set_ylabel('State Value')\n",
    "axs[1,2].grid(axis='y', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:40.084007Z",
     "start_time": "2023-11-19T21:32:39.906974Z"
    }
   },
   "outputs": [],
   "source": [
    "MA_total_reward = pd.DataFrame(total_reward)\n",
    "\n",
    "Rolling_total_reward = MA_total_reward.rolling(window=50).mean()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(MA_total_reward, label='Episode')\n",
    "plt.plot(Rolling_total_reward, color='r', label ='MA(50)')\n",
    "plt.title('Total Rewards')\n",
    "plt.ylabel('Rewards [$]')\n",
    "plt.xlabel('Episodes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - Reinforcement Learning for Effective Clinical Trials\n",
    "\n",
    "- Multi-armed bandits (MAB) is a Reinforcement Learning (RL) problem that has wide applications and is gaining popularity.\n",
    "- Multi-armed bandits extend RL by ignoring the state and try to balance between exploration and exploitation.\n",
    "- Website design and clinical trials are some areas where MAB algorithms are frequently applied.\n",
    "- Contextual bandits takes MAB further by adding an element of Context from the problem domain.\n",
    "- Contextual bandits can improve effectiveness of MAB problems but also be applied to new areas like Recommender systems.\n",
    "\n",
    "https://www.infoq.com/articles/multi-armed-bandits-reinforcement-learning/\n",
    "\n",
    "- At first step, will be created a simple simulated multi-armed bandit problem simulator class. \n",
    "- It will define a 2-armed bandit with each arm pull generating a reward of either 0 or 1. \n",
    "- We will design the simulation such that arm1 will give us maximum rewards. \n",
    "- We will draw 1000 samples by drawing each arm and plot the distribution of rewards in a figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:42.912808Z",
     "start_time": "2023-11-19T21:32:42.905807Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "# simulate the bandit problem\n",
    "class MABandit_Problem:\n",
    "  def __init__(self):\n",
    "    # we build two bandits\n",
    "    self.samples = {}\n",
    "    self.num_samples = 1000\n",
    "    # create samples for both arms\n",
    "    self.samples[0] = np.random.normal(0, 0.1, self.num_samples)\n",
    "    self.samples[0] = self.samples[0] < 0.1\n",
    "    self.samples[1] = np.random.normal(0.2, 0.3, self.num_samples)\n",
    "    self.samples[1] = self.samples[1] < 0.1\n",
    "    self.pointer = 0\n",
    "  # method for acting on the bandits - return reward\n",
    "  def pull_arm(self, k):\n",
    "    reward = 0\n",
    "    # arms should be 0 or 1\n",
    "    if k<0 or k>1:\n",
    "      return -1\n",
    "    # get the reward\n",
    "    reward = self.samples[k][self.pointer]\n",
    "    # update pointer\n",
    "    self.pointer += 1\n",
    "    # rotate the pointer\n",
    "    if self.pointer >= self.num_samples:\n",
    "      self.pointer = 0\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:45.323114Z",
     "start_time": "2023-11-19T21:32:45.060315Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mab = MABandit_Problem() \n",
    "rewards1 = []\n",
    "rewards2 = []\n",
    "# sample the two arms\n",
    "for _ in range(100):\n",
    "  rewards1.append(mab.pull_arm(0))\n",
    "  rewards2.append(mab.pull_arm(1))\n",
    "# plot the rewards for each arm\n",
    "# reward is 0 or 1\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)\n",
    "fig, ax = plt.subplots(2)\n",
    "line1, = ax[0].plot(rewards1, linestyle='None', marker='o')\n",
    "ax[0].set_title('Rewards distribution for Arm 1')\n",
    "line2, = ax[1].plot(rewards2, linestyle='None', marker='o')\n",
    "ax[1].set_title('Rewards distribution for Arm 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:46.488617Z",
     "start_time": "2023-11-19T21:32:46.481176Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "# Classic A/B testing - Pure exploration\n",
    "class AB_Testing:\n",
    "  # initialize bandit class\n",
    "  def __init__(self):\n",
    "    self.mab = MABandit_Problem()\n",
    "    self.total_runs = 1000\n",
    "    self.arm_flag = False\n",
    "    self.arm_rewards = [0, 0]\n",
    "    self.arm_counts = [0, 0]\n",
    "    self.arm_rew_rate = [0, 0]\n",
    "  # method for acting on the bandits - return reward\n",
    "  def run_test(self):\n",
    "    # for each experiment\n",
    "    for _ in range(self.total_runs):  \n",
    "      # pull a random arm\n",
    "      arm = int(self.arm_flag)\n",
    "      self.arm_flag = True if not self.arm_flag else False\n",
    "      # calculate reward\n",
    "      self.arm_rewards[arm] += self.mab.pull_arm(arm)\n",
    "      self.arm_counts[arm] += 1\n",
    "    # calculate total rate of reward\n",
    "    self.arm_rew_rate = np.divide(self.arm_rewards, self.arm_counts)\n",
    "    print(\"Number of each arm pulls = \", self.arm_counts)\n",
    "    print(\"Rate of reward for each arm = \", self.arm_rew_rate)\n",
    "    print('-'*100)\n",
    "    print(\"Finding: Arm 1 gives maximum reward - we only end up running %d runs through Arm 2\"%(self.arm_counts[1]))\n",
    "# run the test\n",
    "test = AB_Testing()\n",
    "test.run_test()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T21:32:47.537882Z",
     "start_time": "2023-11-19T21:32:47.521966Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    " \n",
    "# Epsilon greedy - explore vs exploit trade-off\n",
    "class EPS_Greedy_Testing:\n",
    "  def __init__(self):\n",
    "    self.mab = MABandit_Problem()\n",
    "    self.total_runs = 1000\n",
    "    self.epsilon = 0.2\n",
    "    self.arm_rewards = [0, 0]\n",
    "    self.arm_counts = [0, 0]\n",
    "    self.arm_rew_rate = [0, 0]\n",
    "  # method for acting on the bandits - return reward\n",
    "  def run_test(self):\n",
    "    # for each experiment\n",
    "    for _ in range(self.total_runs):\n",
    "      arm = -1\n",
    "      # pull arm following epsilon greedy policy\n",
    "      if random.random() < self.epsilon:\n",
    "        # exploration - randomly select arm\n",
    "        arm = random.randrange(2)\n",
    "      else:\n",
    "        # exploitation - choose max reward rate arm\n",
    "        arm = np.argmax(self.arm_rew_rate)\n",
    "      # add reward\n",
    "      self.arm_rewards[arm] += self.mab.pull_arm(arm)\n",
    "      self.arm_counts[arm] += 1      \n",
    "      # calculate rate of reward\n",
    "      self.arm_rew_rate = np.divide(self.arm_rewards, self.arm_counts)\n",
    "    print(\"Number of each arm pulls = \", self.arm_counts)\n",
    "    print(\"Rate of reward for each arm = \", self.arm_rew_rate)\n",
    "    print('-'*100)\n",
    "    print(\"Finding: Arm 1 gives maximum reward - we only end up running %d runs through Arm 2\"%(self.arm_counts[1]))\n",
    " \n",
    "test = EPS_Greedy_Testing()\n",
    "test.run_test()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3 - Diabetes simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Type-1 Diabetes simulator implemented in Python for Reinforcement Learning purpose\n",
    "\n",
    "- **simglucose** library\n",
    "- This simulator is a python implementation of the FDA-approved [UVa/Padova Simulator (2008 version)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4454102/) for research purpose only. \n",
    "- The simulator includes 30 virtual patients, 10 adolescents, 10 adults, 10 children.\n",
    "\n",
    "- Jinyu Xie. Simglucose v0.2.1 (2018). Available: https://github.com/jxx123/simglucose. \n",
    "\n",
    "\n",
    "- **simglucose** supports gymnasium! \n",
    "\n",
    "\n",
    "### Main Features\n",
    "\n",
    "- Simulation environment follows [OpenAI gym](https://github.com/openai/gym) and [rllab](https://github.com/rll/rllab) APIs. \n",
    "- It returns observation, reward, done, info at each step, which means the simulator is \"reinforcement-learning-ready\".\n",
    "- Supports customized reward function. \n",
    "- The reward function is a function of blood glucose measurements in the last hour. \n",
    "- By default, the reward at each step is `risk[t-1] - risk[t]`. `risk[t]` is the risk index at time `t` defined in this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2903980/pdf/dia.2008.0138.pdf).\n",
    "- Supports parallel computing. The simulator simulates multiple patients in parallel using [pathos multiprocessing package](https://github.com/uqfoundation/pathos) (you are free to turn parallel off by setting `parallel=False`).\n",
    "- The simulator provides a random scenario generator (`from simglucose.simulation.scenario_gen import RandomScenario`) and a customized scenario generator (`from simglucose.simulation.scenario import CustomScenario`). \n",
    "- **Command line user-interface will guide you through the scenario settings**.\n",
    "- The simulator provides the most basic basal-bolus controller for now. \n",
    "- It provides very simple syntax to implement your own controller, like Model Predictive Control, PID control, reinforcement learning control, etc.\n",
    "- You can specify random seed in case you want to repeat your experiments.\n",
    "- The simulator will generate several plots for performance analysis after simulation. \n",
    "- The plots include blood glucose trace plot, Control Variability Grid Analysis (CVGA) plot, statistics plot of blood glucose in different zones, risk indices statistics plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:25:51.529650Z",
     "start_time": "2023-11-19T19:25:51.525648Z"
    }
   },
   "outputs": [],
   "source": [
    "## Installation\n",
    "#!pip install simglucose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:27:19.921382Z",
     "start_time": "2023-11-19T19:25:54.663710Z"
    }
   },
   "outputs": [],
   "source": [
    "## Quick Start\n",
    "\n",
    "### Use simglucose as a simulator and test controllers\n",
    "\n",
    "# Run the simulator user interface\n",
    "\n",
    "from simglucose.simulation.user_interface import simulate\n",
    "simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:30:19.534058Z",
     "start_time": "2023-11-19T19:27:43.334013Z"
    }
   },
   "outputs": [],
   "source": [
    "## You are free to implement your own controller, and test it in the simulator. For example,\n",
    "\n",
    "\n",
    "from simglucose.simulation.user_interface import simulate\n",
    "from simglucose.controller.base import Controller, Action\n",
    "\n",
    "\n",
    "class MyController(Controller):\n",
    "    def __init__(self, init_state):\n",
    "        self.init_state = init_state\n",
    "        self.state = init_state\n",
    "\n",
    "    def policy(self, observation, reward, done, **info):\n",
    "        '''\n",
    "        Every controller must have this implementation!\n",
    "        ----\n",
    "        Inputs:\n",
    "        observation - a namedtuple defined in simglucose.simulation.env. For\n",
    "                      now, it only has one entry: blood glucose level measured\n",
    "                      by CGM sensor.\n",
    "        reward      - current reward returned by environment\n",
    "        done        - True, game over. False, game continues\n",
    "        info        - additional information as key word arguments,\n",
    "                      simglucose.simulation.env.T1DSimEnv returns patient_name\n",
    "                      and sample_time\n",
    "        ----\n",
    "        Output:\n",
    "        action - a namedtuple defined at the beginning of this file. The\n",
    "                 controller action contains two entries: basal, bolus\n",
    "        '''\n",
    "        self.state = observation\n",
    "        action = Action(basal=0, bolus=0)\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset the controller state to inital state, must be implemented\n",
    "        '''\n",
    "        self.state = self.init_state\n",
    "\n",
    "\n",
    "ctrller = MyController(0)\n",
    "simulate(controller=ctrller)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:30:36.153183Z",
     "start_time": "2023-11-19T19:30:36.150178Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can specify a lot more simulation parameters through `simulation`:\n",
    "\n",
    "#simulate(sim_time=my_sim_time,\n",
    "#         scenario=my_scenario,\n",
    "#        controller=my_controller,\n",
    "#        start_time=my_start_time,\n",
    "#        save_path=my_save_path,\n",
    "#        animate=False,\n",
    "#        parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:30:37.174720Z",
     "start_time": "2023-11-19T19:30:37.168724Z"
    }
   },
   "outputs": [],
   "source": [
    "### OpenAI Gym usage\n",
    "\n",
    "# Using default reward\n",
    "\n",
    "import gym\n",
    "\n",
    "# Register gym environment. By specifying kwargs,\n",
    "# you are able to choose which patient or patients to simulate.\n",
    "# patient_name must be 'adolescent#001' to 'adolescent#010',\n",
    "# or 'adult#001' to 'adult#010', or 'child#001' to 'child#010'\n",
    "# It can also be a list of patient names\n",
    "# You can also specify a custom scenario or a list of custom scenarios\n",
    "# If you chose a list of patient names or a list of custom scenarios,\n",
    "# every time the environment is reset, a random patient and scenario will be\n",
    "# chosen from the list\n",
    "\n",
    "from gym.envs.registration import register\n",
    "from simglucose.simulation.scenario import CustomScenario\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:34:12.673467Z",
     "start_time": "2023-11-19T19:34:12.668891Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = datetime(2023, 11, 19, 0, 0, 0)\n",
    "meal_scenario = CustomScenario(start_time=start_time, scenario=[(1,50)])\n",
    "\n",
    "#register(\n",
    "#    id='simglucose-adolescent2-v0',\n",
    "#    entry_point='simglucose.envs:T1DSimEnv',\n",
    "#    kwargs={'patient_name': 'adolescent#002',\n",
    "#           'custom_scenario': meal_scenario}\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:34:13.908617Z",
     "start_time": "2023-11-19T19:34:13.893445Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('simglucose-adolescent2-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:34:25.099931Z",
     "start_time": "2023-11-19T19:34:15.151170Z"
    }
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "for t in range(100):\n",
    "    env.render(mode='human')\n",
    "    print(observation)\n",
    "    # Action in the gym environment is a scalar\n",
    "    # representing the basal insulin, which differs from\n",
    "    # the regular controller action outside the gym\n",
    "    # environment (a tuple (basal, bolus)).\n",
    "    # In the perfect situation, the agent should be able\n",
    "    # to control the glucose only through basal instead\n",
    "    # of asking patient to take bolus\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t + 1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:35:44.281989Z",
     "start_time": "2023-11-19T19:35:33.574101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Customized reward function\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "\n",
    "def custom_reward(BG_last_hour):\n",
    "    if BG_last_hour[-1] > 180:\n",
    "        return -1\n",
    "    elif BG_last_hour[-1] < 70:\n",
    "        return -2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('simglucose-adolescent2-v0')\n",
    "\n",
    "reward = 1\n",
    "done = False\n",
    "\n",
    "observation = env.reset()\n",
    "for t in range(200):\n",
    "    env.render(mode='human')\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(observation)\n",
    "    print(\"Reward = {}\".format(reward))\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t + 1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
