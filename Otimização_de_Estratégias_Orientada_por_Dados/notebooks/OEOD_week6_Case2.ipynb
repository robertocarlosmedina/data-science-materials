{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acc5fcc",
   "metadata": {},
   "source": [
    "<font size=\"5\">\n",
    " <div class=\"alert alert-block alert-info\"><b>Master in Data Science - Iscte <b>\n",
    "     </div>\n",
    "</font> \n",
    " \n",
    " \n",
    "     \n",
    "    \n",
    "  <font size=\"5\"> OEOD </font>\n",
    "  \n",
    "  \n",
    "  \n",
    "  <font size=\"3\"> **Diana Aldea Mendes**, October 2023 </font>\n",
    "  \n",
    "   \n",
    "  <font size=\"3\"> *diana.mendes@iscte-iul.pt* </font> \n",
    "  \n",
    "    \n",
    " \n",
    "  \n",
    "    \n",
    "  <font color='blue'><font size=\"5\"> <b>Week 6 - Self-Driving Taxi Problem<b></font></font>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0bebc",
   "metadata": {},
   "source": [
    "# RL libraries\n",
    "\n",
    "- **Open AI Gym** \n",
    "- KerasRL\n",
    "- Tensorflow\n",
    "- RL-Coach\n",
    "- RLkit\n",
    "- Stable Baseline\n",
    "- Dopamine\n",
    "- TF Agents\n",
    "\n",
    "# Open AI Gym\n",
    "\n",
    "- OpenAI **Gym** package: https://gymnasium.farama.org/\n",
    "    - can be used to build RL algorithms\n",
    "    - several environments are available - providing a state space and an action space, along with the rewards and outcome responses\n",
    "    - you can also construct a **new** environment (*custom environment*)\n",
    "    - Work Example in this notebook: **Taxi-v3 task**\n",
    "    - `gym.make()` - create the environment and returns the object\n",
    "    - `env.reset()` - resets the environment's state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f2746",
   "metadata": {},
   "source": [
    "## Example - Taxi task\n",
    "\n",
    "- **Reference**: *Habib, N. (2019), Hands-on Q-Learning with Python. Practical Q-Learning with OpenAI Gym, Keras and Tensorflow, Packt.*\n",
    "\n",
    "- **Goal**: a self-driving taxi with the task to collect passenger(s) from a starting location and drop them off at their desired destination in the fewest steps possible\n",
    "\n",
    "\n",
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbe772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:44:06.689321Z",
     "start_time": "2023-10-31T16:44:06.674317Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571a590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:44:07.165331Z",
     "start_time": "2023-10-31T16:44:07.160324Z"
    }
   },
   "outputs": [],
   "source": [
    "print(state)\n",
    "\n",
    "# the value of state; it will be a different random value between 0 and 499 every time we run env.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6963274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e769940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:44:09.005261Z",
     "start_time": "2023-10-31T16:44:09.000263Z"
    }
   },
   "outputs": [],
   "source": [
    "## visualize the environment, \n",
    "# yellow rectangle = taxi is free\n",
    "# green rectangle = taxi has a passenger\n",
    "# letter = destinations (to pick-up or drop passenger)\n",
    "\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f525c064",
   "metadata": {},
   "source": [
    "### States and Actions\n",
    "\n",
    "- Each state variable is characterized by:\n",
    "    - Where the taxi is now (out of 25 possibilities)\n",
    "    - Where the passenger is now (inside the taxi or at one of the four locations marked R, G, B, or Y)\n",
    "    - Where the passenger's destination is (R, G, B, or Y)\n",
    "- This gives us 25 x 5 x 4 = 500 distinct states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd98c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:51.012375Z",
     "start_time": "2023-10-31T16:13:51.001377Z"
    }
   },
   "outputs": [],
   "source": [
    "# run again 'reset()' function\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c12d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:51.583817Z",
     "start_time": "2023-10-31T16:13:51.568821Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.render())\n",
    "# the taxi agent has moved to a different random location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3e0c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:52.087213Z",
     "start_time": "2023-10-31T16:13:52.066218Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of states from the environment\n",
    "env.observation_space\n",
    "#print('State Space {}'.format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cecb35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:52.597198Z",
     "start_time": "2023-10-31T16:13:52.577203Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of actions from the environment\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ab347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:53.151265Z",
     "start_time": "2023-10-31T16:13:53.144270Z"
    }
   },
   "outputs": [],
   "source": [
    "# which are the 6 actions???\n",
    "# 0: South\n",
    "# 1: North\n",
    "# 2: East\n",
    "# 3: West\n",
    "# 4: Pickup\n",
    "# 5: Drop-off\n",
    "\n",
    "\n",
    "# generate a valid action from the action space (randomly selects an action)\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284ba9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:53.709870Z",
     "start_time": "2023-10-31T16:13:53.696871Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose an action manually (1 - move north)\n",
    "env.step(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dbce3",
   "metadata": {},
   "source": [
    "######################################################################\n",
    "\n",
    "- `env.step(1)` returns the following four variables:\n",
    "    - observation: This refers to the new state that we are in (that is, state 232).\n",
    "    - reward: This refers to the reward that we have received (-1).\n",
    "    - done: This tells us whether we have successfully dropped off the passenger at the correct location (False).\n",
    "    - truncated: True if episode truncates due to a time limit or other reason (False)\n",
    "    - info: This provides any additional information that we may need for debugging.\n",
    "- Usually, we are **not setting the action values manually**; instead, we will let the algorithm that we are running to choose them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78dd0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:54.745860Z",
     "start_time": "2023-10-31T16:13:54.736861Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7308e",
   "metadata": {},
   "source": [
    "### Random Agent\n",
    "\n",
    "- In this case, all agents will take random actions and does not keep track of its actions or learn from them\n",
    "- This is the **baseline agent** and serves as a control to compare the performance of other RL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a06cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:55.815022Z",
     "start_time": "2023-10-31T16:13:55.801022Z"
    }
   },
   "outputs": [],
   "source": [
    "## random action - with 'env.action_space.sample()' function\n",
    "# returns a random action\n",
    "\n",
    "env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d2cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:56.402091Z",
     "start_time": "2023-10-31T16:13:56.394091Z"
    }
   },
   "outputs": [],
   "source": [
    "# send the random action to the next state\n",
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7e559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:56.950366Z",
     "start_time": "2023-10-31T16:13:56.937344Z"
    }
   },
   "outputs": [],
   "source": [
    "observation, reward, done, truncate, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703eeaf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:57.505446Z",
     "start_time": "2023-10-31T16:13:57.498451Z"
    }
   },
   "outputs": [],
   "source": [
    "## consider state = 50\n",
    "env.env.s=50\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ce3dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:58.093111Z",
     "start_time": "2023-10-31T16:13:58.073056Z"
    }
   },
   "outputs": [],
   "source": [
    "# example (one action, one state change)\n",
    "# take action 0 (move south)\n",
    "env.step(0)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce96afb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:58.610478Z",
     "start_time": "2023-10-31T16:13:58.593475Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af10e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:13:59.165172Z",
     "start_time": "2023-10-31T16:13:59.153175Z"
    }
   },
   "outputs": [],
   "source": [
    "# observation: 452 - we are in state 452\n",
    "# reward: -1\n",
    "# done: False - do not reached the destination\n",
    "# truncate: False\n",
    "# info: {'prob': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1c767",
   "metadata": {},
   "source": [
    "#### Creating a task loop\n",
    "- The agent moves randomly until successfully reaches the goal\n",
    "- Goal: drop the passenger at the correct location\n",
    "- We simply use the basic Gym functions presented before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4f8d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:00.521030Z",
     "start_time": "2023-10-31T16:14:00.465005Z"
    }
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "reward = 0\n",
    "while reward != 20:\n",
    "    observation, reward, done, truncate, info = env.step(env.action_space.sample())\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a447a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:01.273912Z",
     "start_time": "2023-10-31T16:14:01.265908Z"
    }
   },
   "outputs": [],
   "source": [
    "# reward = 0 , the goal was not reached\n",
    "# reward !=20 - this is the ending condition, so we loop until the reward is less than 20\n",
    "# the taxi drop-off the passenger at location B and receive the 20 points reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0994021",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:01.890253Z",
     "start_time": "2023-10-31T16:14:01.880149Z"
    }
   },
   "outputs": [],
   "source": [
    "# verify if the task was reached (use done output from 'env.step()')\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3bfd3",
   "metadata": {},
   "source": [
    "########################################################\n",
    "- At this point, we know that the agent has taken a series of random actions and has eventually reached the goal. \n",
    "- But what actions did it actually take? \n",
    "- How many steps did it take to get to the destination?\n",
    "- All this information is important to compare with other algorithms and analyze their performance and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a40acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:03.823093Z",
     "start_time": "2023-10-31T16:14:03.704730Z"
    }
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "count = 0\n",
    "reward = 0\n",
    "while reward != 20:\n",
    "    observation, reward, done, truncate, info = env.step(env.action_space.sample())\n",
    "    count += 1\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175ae60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:05.177527Z",
     "start_time": "2023-10-31T16:14:05.160527Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of steps to reach the goal\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc922943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:06.146009Z",
     "start_time": "2023-10-31T16:14:06.066734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, we want to know each action that the agent takes during the loop\n",
    "# render the game environment at each step, based on how many steps it takes for the agent to reach the destination.\n",
    "##### very dense output \n",
    "\n",
    "observation = env.reset()\n",
    "count = 0\n",
    "reward = 0\n",
    "while reward != 20:\n",
    "    observation, reward, done, truncate, info = env.step(env.action_space.sample())\n",
    "    count += 1\n",
    "    print(env.render()) #render each step of the game loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e0071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:10.742576Z",
     "start_time": "2023-10-31T16:14:10.650058Z"
    }
   },
   "outputs": [],
   "source": [
    "## supose now, that we only want to see the information related to the action taken at each step\n",
    "# so, call 'env.action_space.sample()' insteed of 'env.render()'\n",
    "\n",
    "observation = env.reset()\n",
    "count = 0\n",
    "reward = 0\n",
    "while reward != 20:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, truncate, info = env.step(action)\n",
    "    count += 1\n",
    "    print(action)\n",
    "    #print(action, end=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9001dec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:12.714883Z",
     "start_time": "2023-10-31T16:14:12.695882Z"
    }
   },
   "outputs": [],
   "source": [
    "# define dictionary (action number : action description)\n",
    "taxi_actions = {0 : 'South',1 : 'North',2 : 'East',3 : 'West',4 : 'Pickup',5 : 'Dropoff'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50338c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:13.584632Z",
     "start_time": "2023-10-31T16:14:13.569631Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi_actions.get(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee611a",
   "metadata": {},
   "source": [
    "### Q-learning Agent\n",
    "\n",
    "- **Q-learning** *agent* with a smart-taxi (self-driving taxi), discrete-state environment with small state space.\n",
    "- **Goal**: collect passenger(s) from a starting location and drop them off at their desired destination in the fewest steps possible\n",
    "- The taxi  collects a reward when it drops off a passenger and gets penalties for taking other actions\n",
    "- All rewards are stored in the Q-table (maps states to actions)\n",
    "- **Gym provides the environment, the actions, the states**\n",
    "- *We have to provide the Q-learning algorithm that finds the optimal solution*\n",
    "- *Using Gym will allow you to build reinforcement learning models, compare their performance, keep track of updated versions, and share your work*.\n",
    "\n",
    "- Task to do:\n",
    "    - Understanding how the agent updates the Q-table and uses it to make decisions\n",
    "    - Adapting the appropriate Bellman equation to update the Q-table with each action\n",
    "    - Understanding the role of the learning parameter (alpha) in the Bellman equation and what happens when they are adjusted\n",
    "    - Implementing epsilon decay to improve the performance of your agent\n",
    "    \n",
    "- When the algorithm is complete, the agent will start out with no knowledge of the taxi environment and will quickly learn the rules that get it the highest rewards through exploration of the environment.\n",
    "- During this process, the agent start to reach its goal more quickly and efficiently, and it will learn to do this without being explicitly programmed to do so. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac53968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:14.710159Z",
     "start_time": "2023-10-31T16:14:14.697154Z"
    }
   },
   "outputs": [],
   "source": [
    "## import again the libraries and the environment\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db38242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:15.374669Z",
     "start_time": "2023-10-31T16:14:15.362668Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the number of states and actions\n",
    "\n",
    "print(\"Number of actions: %d\" % env.action_space.n)\n",
    "print(\"Number of states: %d\" % env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4444b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:16.440915Z",
     "start_time": "2023-10-31T16:14:16.428763Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the Q-table (with all entries = zero)\n",
    "# Q-table is a two-dimensional numpy array (matrix)\n",
    "# the first column = state\n",
    "# the remaining 6 columns = the 6 possible actions\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29915a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:17.056561Z",
     "start_time": "2023-10-31T16:14:17.040184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, for example, if we are in state 1 and decided to take action 2 (East), and the Q-value we calculate for\n",
    "# this state-action pair is -1, we would update the Q-table.\n",
    "\n",
    "# When the agent returns to state 1 again, it will look up the row in the Q-table for state 1 to get the action values. \n",
    "# When it does, it will see that the action with the lowest Q-value is currently action 2 (since is -1, others are 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac3fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:17.642089Z",
     "start_time": "2023-10-31T16:14:17.632087Z"
    }
   },
   "outputs": [],
   "source": [
    "## set hyperparameters\n",
    "# discount rate\n",
    "\n",
    "gamma = 0.9               \n",
    "\n",
    "# initialize reward\n",
    "reward =0\n",
    "\n",
    "# initialize environment (initial random state)\n",
    "state = env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62631f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:18.241749Z",
     "start_time": "2023-10-31T16:14:18.231736Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 very important functions !!!!!\n",
    "\n",
    "## returns the index of the maximum value in the <state> row of the Q-table\n",
    "action = np.argmax(Q[state])  \n",
    "\n",
    "# actualize state\n",
    "next_state, reward, done, truncate, info = env.step(action)\n",
    "\n",
    "# actualize q-table\n",
    "Q[state,action] = reward + gamma * np.max(Q[next_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc80b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:18.891301Z",
     "start_time": "2023-10-31T16:14:18.849794Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "# Define Bellman equation - select action with the current highest Q-value (simple greedy strategy)\n",
    "# Q[state, action] = reward + gamma * np.max(Q[next_state])\n",
    "\n",
    "# create update loop\n",
    "\n",
    "#############################################\n",
    "\n",
    "while reward != 20: \n",
    "    #choose current highest-valued action\n",
    "    action = np.argmax(Q[state])\n",
    "    #obtain reward and next state resulting from taking action\n",
    "    next_state, reward, done, truncate, info = env.step(action)\n",
    "    #update Q-value for state-action pair\n",
    "    Q[state, action] = reward + gamma * np.max(Q[next_state])\n",
    "    #update state\n",
    "    state = next_state\n",
    "\n",
    "#render final dropoff state\n",
    "print(env.render())  ## this is the final state the environment reaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703de52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:19.590834Z",
     "start_time": "2023-10-31T16:14:19.528512Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## The same as before, but also counting the number of steps\n",
    "## This is important in order to compare with other algorithms / strategies\n",
    "\n",
    "##########################################################\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "gamma = 0.1\n",
    "state = env.reset()[0]\n",
    "count = 0\n",
    "reward = 0\n",
    "while reward != 20:\n",
    "    action = np.argmax(Q[state])\n",
    "    next_state, reward, done, truncate, info = env.step(action)\n",
    "    Q[state, action] = reward + gamma * np.max(Q[next_state])\n",
    "    state = next_state\n",
    "    count += 1\n",
    "    \n",
    "print(env.render())\n",
    "print('Counter: {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d5aa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:20.120768Z",
     "start_time": "2023-10-31T16:14:20.104685Z"
    }
   },
   "outputs": [],
   "source": [
    "### counter = 885, so the agent takes 885 random steps before dropping off the passenger at correct location\n",
    "\n",
    "### running the loop several times - lead to similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f902906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:14:20.709535Z",
     "start_time": "2023-10-31T16:14:20.673468Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "\n",
    "## adding more hyperparameters (alpha and epsilon)\n",
    "\n",
    "## adding and updating alpha (learning rate) \n",
    "# Note that the Bellman equation change the expression\n",
    "\n",
    "################################################\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "gamma = 0.1\n",
    "alpha = 0.1\n",
    "state = env.reset()[0]\n",
    "count = 0\n",
    "reward = 0\n",
    "\n",
    "while reward != 20:\n",
    "    action = np.argmax(Q[state])\n",
    "    next_state, reward, done, truncate, info = env.step(action)\n",
    "    Q[state, action] = Q[state, action] + alpha * (reward + gamma * \\\n",
    "    np.max(Q[next_state]) - Q[state, action])\n",
    "    state = next_state\n",
    "    count += 1\n",
    "    \n",
    "print(env.render())\n",
    "print('Counter: {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f3c40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T23:46:50.543163Z",
     "start_time": "2023-10-30T23:46:50.503150Z"
    }
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "\n",
    "## adding and updating epsilon\n",
    "## Now, the agent has the ability to explore new actions it hasn't taken yet and balance out its exploitation of the high-valued actions it's already taken.\n",
    "\n",
    "## in this case, we only add the epsilon-greedy rule into the update loop\n",
    "# the Q-table update rule (Bellman eq. ) is the same as before\n",
    "\n",
    "##################################################\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "gamma = 0.1\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "state = env.reset()[0]\n",
    "count = 0\n",
    "reward = 0\n",
    "\n",
    "while reward != 20:\n",
    "    if np.random.rand() < epsilon:\n",
    "    #exploration option\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        #exploitation option\n",
    "        action = np.argmax(Q[state])\n",
    "    next_state, reward, done, truncate, info = env.step(action)\n",
    "    Q[state, action] = Q[state, action] + alpha * (reward + gamma * \\\n",
    "    np.max(Q[next_state]) - Q[state, action])\n",
    "    state = next_state\n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "print(env.render())\n",
    "print('Counter: {}'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ec9bc",
   "metadata": {},
   "source": [
    "### Performance measures and comparing models\n",
    "\n",
    "- Note that, you can test and compare different values for epsilon ans alpha - as part of model-tuning process\n",
    "- Now, we need to test their performance and make sure they are improving with respect to speed and accuracy.\n",
    "- In what follows, we test the performance of our baseline agent (random-acting agent) against our Q-learning agent model and observe what happens when we run the model over more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6edfc77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T23:48:36.293800Z",
     "start_time": "2023-10-30T23:48:36.158850Z"
    }
   },
   "outputs": [],
   "source": [
    "#### random-acting agent algorithm\n",
    "# change counter (count) with epochs - to distinguish each training time step from each full game loop\n",
    "# cycle the agent completes:\n",
    "\n",
    "state = env.reset()\n",
    "epochs = 0\n",
    "reward = 0\n",
    "while reward != 20:\n",
    "    state, reward, done, truncate, info = env.step(env.action_space.sample())\n",
    "    epochs += 1\n",
    "    \n",
    "\n",
    "env.render()\n",
    "print(\"Timesteps taken: {}\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88e204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T23:49:28.831500Z",
     "start_time": "2023-10-30T23:49:22.592220Z"
    }
   },
   "outputs": [],
   "source": [
    "### construct an episode loop and run 100 episodes\n",
    "\n",
    "# We calculate the average number of time steps by dividing the total number of epochs per\n",
    "# game iteration (time step) by the total number of episodes (number of game iterations run)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "total_epochs = 0\n",
    "episodes = 100\n",
    "for episode in range(episodes):\n",
    "    epochs = 0\n",
    "    reward = 0\n",
    "    state = env.reset()\n",
    "    while reward != 20:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, truncate, info = env.step(action)\n",
    "    epochs += 1\n",
    "    total_epochs += epochs\n",
    "    \n",
    "print(\"Average timesteps taken: {}\".format(total_epochs/episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340032ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T23:55:03.951283Z",
     "start_time": "2023-10-30T23:55:01.321209Z"
    }
   },
   "outputs": [],
   "source": [
    "### now, we do the same, for the Q-learning agent\n",
    "## observe that we obtain less timesteps and so, improve the results\n",
    "\n",
    "##########################################\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "gamma = 0.1\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "total_epochs = 0\n",
    "episodes = 100\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    epochs=0\n",
    "    reward=0\n",
    "    state = env.reset()[0]  \n",
    "    while reward != 20:\n",
    "        if np.random.rand() < epsilon:\n",
    "        #exploration option\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            #exploitation option\n",
    "            action = np.argmax(Q[state])\n",
    "        next_state, reward, done, truncate, info = env.step(action)\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * \\\n",
    "        np.max(Q[next_state]) - Q[state, action])\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "    total_epochs +=epochs\n",
    "    \n",
    "    \n",
    "print(\"Average timesteps taken: {}\".format(total_epochs/episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a84b1",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "- As the number of states in a Q-learning task increases, a simple Q-table is no longer a practical way of modeling the state-action transition function. \n",
    "- Instead, we can use a Q-network, which is a type of neural network that is designed to approximate Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e3d2a",
   "metadata": {},
   "source": [
    "## HW \n",
    "### Exercise 1\n",
    "\n",
    "- Run the random agent for 1000 episodes (or 10000 episodes if you have o good computer, since it takes some time to run). Which is your conclusion? You improved the results by increasing the training?\n",
    "- Do the same as before for the Q-learning agent.\n",
    "- What happens when you increase the number of episodes to 100000 (Q-learning)? Does the agent's performance get better or worse, or does it stay the same?\n",
    "\n",
    "### Exercise 2\n",
    "- Tune your Q-learning algorithm, by considering different values for alpha and gamma \n",
    "    - for example fix alpha =0.01 and vary gamma between 0.1 and 0.9\n",
    "    - for example fix gamma =0.1 and vary alpha between 0.1 and 0.9\n",
    "    - for example fix alpha =0.9 and vary gamma between 0.01 and 0.1\n",
    "    - for example fix gamma =0.5 and vary alpha between 0.01 and 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1c92f",
   "metadata": {},
   "source": [
    "## Extra - Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619786b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:44:28.864839Z",
     "start_time": "2023-10-31T16:44:28.858838Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from matplotlib import animation\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b738be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:44:30.178826Z",
     "start_time": "2023-10-31T16:44:29.984829Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Initialize and validate the environment\"\"\"\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\").env\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Print dimensions of state and action space\n",
    "print(\"State space: {}\".format(env.observation_space))\n",
    "print(\"Action space: {}\".format(env.action_space))\n",
    "\n",
    "# Sample random action\n",
    "action = env.action_space.sample(env.action_mask(state))\n",
    "next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "# Print output\n",
    "print(\"State: {}\".format(state))\n",
    "print(\"Action: {}\".format(action))\n",
    "print(\"Action mask: {}\".format(env.action_mask(state)))\n",
    "print(\"Reward: {}\".format(reward))\n",
    "\n",
    "# Render and plot an environment frame\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650dbd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:44:34.518898Z",
     "start_time": "2023-10-31T16:44:34.512899Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_animation(experience_buffer):\n",
    "    \"\"\"Function to run animation\"\"\"\n",
    "    time_lag = 0.05  # Delay (in s) between frames\n",
    "    for experience in experience_buffer:\n",
    "        # Plot frame\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(experience['frame'])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # Print console output\n",
    "        print(f\"Episode: {experience['episode']}/{experience_buffer[-1]['episode']}\")\n",
    "        print(f\"Epoch: {experience['epoch']}/{experience_buffer[-1]['epoch']}\")\n",
    "        print(f\"State: {experience['state']}\")\n",
    "        print(f\"Action: {experience['action']}\")\n",
    "        print(f\"Reward: {experience['reward']}\")\n",
    "        # Pauze animation\n",
    "        sleep(time_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d639bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:44:48.143223Z",
     "start_time": "2023-10-31T16:44:48.136201Z"
    }
   },
   "outputs": [],
   "source": [
    "def store_episode_as_gif(experience_buffer, path='./', filename='animation.gif'):\n",
    "    \"\"\"Store episode as gif animation\"\"\"\n",
    "    fps = 5   # Set framew per seconds\n",
    "    dpi = 300  # Set dots per inch\n",
    "    interval = 50  # Interval between frames (in ms)\n",
    "\n",
    "    # Retrieve frames from experience buffer\n",
    "    frames = []\n",
    "    for experience in experience_buffer:\n",
    "        frames.append(experience['frame'])\n",
    "\n",
    "    # Fix frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Generate animation\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=interval)\n",
    "\n",
    "    # Save output as gif\n",
    "    anim.save(path + filename, writer='imagemagick', fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46709fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T16:46:34.326450Z",
     "start_time": "2023-10-31T16:44:50.832596Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Simulation with random agent\"\"\"\n",
    "epoch = 0\n",
    "num_failed_dropoffs = 0\n",
    "experience_buffer = []\n",
    "cum_reward = 0\n",
    "\n",
    "done = False\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "while not done:\n",
    "    # Sample random action\n",
    "    \"Action selection without action mask\"\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    \"Action selection with action mask\"\n",
    "    #action = env.action_space.sample(env.action_mask(state))\n",
    "\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "\n",
    "    # Store experience in dictionary\n",
    "    experience_buffer.append({\n",
    "        \"frame\": env.render(),\n",
    "        \"episode\": 1,\n",
    "        \"epoch\": epoch,\n",
    "        \"state\": state,\n",
    "        \"action\": action,\n",
    "        \"reward\": cum_reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if reward == -10:\n",
    "        num_failed_dropoffs += 1\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# Run animation and print console output\n",
    "run_animation(experience_buffer)\n",
    "\n",
    "print(\"# epochs: {}\".format(epoch))\n",
    "print(\"# failed drop-offs: {}\".format(num_failed_dropoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7779e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2776280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
