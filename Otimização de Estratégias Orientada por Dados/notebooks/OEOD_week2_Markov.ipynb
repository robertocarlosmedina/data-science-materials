{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acc5fcc",
   "metadata": {},
   "source": [
    "<font size=\"5\">\n",
    " <div class=\"alert alert-block alert-info\"><b>Master in Data Science - Iscte <b>\n",
    "     </div>\n",
    "</font> \n",
    " \n",
    " \n",
    "     \n",
    "    \n",
    "  <font size=\"5\"> OEOD </font>\n",
    "  \n",
    "  \n",
    "  \n",
    "  <font size=\"3\"> **Diana Aldea Mendes**, September 2023 </font>\n",
    "  \n",
    "   \n",
    "  <font size=\"3\"> *diana.mendes@iscte-iul.pt* </font> \n",
    "  \n",
    "    \n",
    " \n",
    "  \n",
    "    \n",
    "  <font color='blue'><font size=\"5\"> <b>Week 2 - Markov Processes and Grid World RL<b></font></font>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0bebc",
   "metadata": {},
   "source": [
    "# Basic overview\n",
    "\n",
    "From [wiki](https://en.wikipedia.org/wiki/Reinforcement_learning): \n",
    "\n",
    "- **Reinforcement learning (RL)** is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward.*\n",
    "\n",
    "- The 4 main components of any RL algorithm are therefore the following:\n",
    "\n",
    "* `Agent` - an entity (computer program) that makes decisions. \n",
    "\n",
    "* `Action` - decision made by an agent. \n",
    "\n",
    "* `Environment` - an interface for the agents to interact with. The environment accepts actions and responds with the result and a new set of observations. \n",
    "\n",
    "* `Reward` - a function that assigns a value (reward) for each action that an agent can take. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd7ad1",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "- In an MDP, we can define three sets: \n",
    "\n",
    "$\\mathbb{S}$ - the set of all possible states \n",
    "\n",
    "$\\mathbb{A}$ - the set of all possible actions\n",
    "\n",
    "$\\mathbb{R}$ - the set of all possible rewards\n",
    "\n",
    "Then, the dynamics of an MDP process can be defined as a probability: \n",
    "\n",
    "$p(s^{*}, r| s, a) = P(S_{t} = s^{*}, R_{t} = r | S_{t-1} = s, A_{t-1}=a)$\n",
    "\n",
    "$S_{t} \\in \\mathbb{S}, R_{t} \\in \\mathbb{R}, A_{t} \\in \\mathbb{A}$  $\\forall t$\n",
    "\n",
    "- Because we are dealing with probabilities, then: \n",
    "\n",
    "$$ \\sum_{s^{*} \\in \\mathbb{S}} \\sum_{r \\in \\mathbb{R}} p(s^{*}, r| s, a) = 1 $$\n",
    "\n",
    "\n",
    "## Returns and rewards\n",
    "\n",
    "- The reward hypothesis states that: \n",
    "\n",
    "> The goal of an agent is to maximize the expected total reward it receives over the course of its lifetime.\n",
    "\n",
    "- The expected return at time step $t$ is denoted as: \n",
    "\n",
    "$$G_{t} = R_{t+1} + R_{t+2} + ... + R_{T}$$ \n",
    "\n",
    "Where $T$ is the terminal time step when the episode ends. \n",
    "\n",
    "- When there is no clear ending to the episode, the return is defined as:\n",
    "\n",
    "$$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + ...$$\n",
    "\n",
    "Where $\\gamma \\in [0, 1]$ is a discount factor.\n",
    "\n",
    "## Policies and value functions \n",
    "\n",
    "- A policy is a mapping (denoted as $\\pi$) from states to probabilities of taking a certain action.\n",
    "\n",
    "$$ \\pi : \\mathbb{S} \\rightarrow \\mathbb{A}$$ \n",
    "\n",
    "Or \n",
    "\n",
    "$$ \\pi(a \\in \\mathbb{A}| s \\in \\mathbb{S}) \\in [0, 1] $$\n",
    "\n",
    "- The value function of a state $s$ under a policy $\\pi$, denoted $v_{\\pi}(s)$, is the expected return when starting in s and following $\\pi$ thereafter. The full definition is: \n",
    "\n",
    "$$ v_{\\pi} (s) = \\mathbb{E}_{\\pi} \\left[ G_{t} | S_{t} = s\\right] =  \\mathbb{E}_{\\pi} \\left[\\sum_{k = 0}^{\\infty}R_{t + k + 1} \\gamma^{k} | S_{t} = s\\right]$$\n",
    "\n",
    "- The action value function, which assigns a value to taking an action $a$ in a state $s$ under a policy $\\pi$, is denoted $q_{\\pi}(s, a)$. The full definition is:\n",
    "\n",
    "$$ q_{\\pi} (s, a) = \\mathbb{E}_{\\pi} \\left[ G_{t} | S_{t} = s, A_{t} = a\\right] =  \\mathbb{E}_{\\pi} \\left[\\sum_{k = 0}^{\\infty}R_{t + k + 1} \\gamma^{k} | S_{t} = s, A_{t} = a\\right]$$\n",
    "\n",
    "- The Bellman equation for state value function is: \n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_{a \\in \\mathbb{A}} \\pi(a|s) \\sum_{s^{*} \\in \\mathbb{S}} \\sum_{r \\in \\mathbb{R}} p(s^{*}, r| s, a) \\left[ r + \\gamma v_{\\pi}(s^{*}) \\right] $$\n",
    "\n",
    "Here \n",
    "\n",
    "$s^{*}$ - the next state from the current state $s$. \n",
    "\n",
    "- The equation states that the value of a state is the `sum of the expected return of all possible actions taken in that state`. \n",
    "- The Belman equation also has a recursive property. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210ece3",
   "metadata": {},
   "source": [
    "# Example - Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64549914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:32:27.128349Z",
     "start_time": "2023-09-26T14:32:27.119319Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy library to do vector algebra\n",
    "import numpy as np\n",
    "\n",
    "# define a transition matrix\n",
    "P = np.array([[0.3, 0.7], [0.2, 0.8]])\n",
    "print(\"Transition Matrix:\\n\", P)\n",
    "\n",
    "# define any starting solution to state probabilities\n",
    "# Here we assume equal probabilities for all the states\n",
    "S = np.array([0.5, 0.5])\n",
    "\n",
    "# run through 10 iterations to calculate steady state\n",
    "# transition probabilities\n",
    "for i in range(10):\n",
    "    S = np.dot(S, P)\n",
    "    print(\"\\nIter {0}. Probability vector S = {1}\".format(i, S))\n",
    "print(\"\\nFinal Vector S={0}\".format(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83424c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:35:52.679573Z",
     "start_time": "2023-09-26T14:35:52.662574Z"
    }
   },
   "outputs": [],
   "source": [
    "#the following code takes a list such as\n",
    "#[1,1,2,6,8,5,5,7,8,8,1,1,4,5,5,0,0,0,1,1,4,4,5,1,3,3,4,5,4,1,1]\n",
    "#with states labeled as successive integers starting with 0\n",
    "#and returns a transition matrix, M,\n",
    "#where M[i][j] is the probability of transitioning from i to j\n",
    "\n",
    "def transition_matrix(transitions):\n",
    "    n = 1+ max(transitions) #number of states\n",
    "\n",
    "    M = [[0]*n for _ in range(n)]\n",
    "\n",
    "    for (i,j) in zip(transitions,transitions[1:]):\n",
    "        M[i][j] += 1\n",
    "\n",
    "    #now convert to probabilities:\n",
    "    for row in M:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    return M\n",
    "\n",
    "#test:\n",
    "\n",
    "t = [1,1,2,6,8,5,5,7,8,8,1,1,4,5,5,0,0,0,1,1,4,4,5,1,3,3,4,5,4,1,1]\n",
    "m = transition_matrix(t)\n",
    "for row in m: print(' '.join('{0:.2f}'.format(x) for x in row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8c85b",
   "metadata": {},
   "source": [
    "# Example - Grid World \n",
    "\n",
    "- The Gridworld problem in `RL` is a problem where we want to create an optimal strategy for an agent to traverse a grid.\n",
    "- A grid is a square matrix of cells, and the agent can move in any of the four directions (up, down, left, right) in each cell. \n",
    "- The agent receives a reward of -1 for each step it takes, and a reward of +10 if it reaches the goal cell.\n",
    "\n",
    "- In this example, there will be 5 goal cells: one in each corner and the in the center. \n",
    "- The agent can start from any non goal squares and has to reach one of the goal cells. \n",
    "- The agent can only move in the four directions, and cannot move diagonally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cf04a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:38:27.411727Z",
     "start_time": "2023-09-26T14:38:27.397797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06261c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:38:28.957569Z",
     "start_time": "2023-09-26T14:38:28.949569Z"
    }
   },
   "outputs": [],
   "source": [
    "def array_index_to_matplot_coords(i: int, j: int, n_cols: int) -> Tuple[int, int]:\n",
    "    \"\"\"Converts an array index to a matplot coordinate\"\"\"\n",
    "    x = j\n",
    "    y = n_cols - i - 1\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b7f83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:38:29.692440Z",
     "start_time": "2023-09-26T14:38:29.654341Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_matrix(\n",
    "    M: np.array, \n",
    "    goal_coords: list = [],\n",
    "    img_width: int = 5, \n",
    "    img_height: int = 5, \n",
    "    title: str = None,\n",
    "    annotate_goal: bool = True\n",
    "    ) -> None: \n",
    "    \"\"\"\n",
    "    Plots a matrix as an image.\n",
    "    \"\"\"\n",
    "    height, width = M.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    \n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # By default, the (0, 0) coordinate in matplotlib is the bottom left corner,\n",
    "            # so we need to invert the y coordinate to plot the matrix correctly\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "                if annotate_goal:\n",
    "                    ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "            else: \n",
    "                ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "\n",
    "    offset = .5    \n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_policy_matrix(P: dict, S:np.array, goal_coords: list = [], img_width: int = 5, img_height: int = 5, title: str = None) -> None: \n",
    "    \"\"\" \n",
    "    Plots the policy matrix out of the dictionary provided; The dictionary values are used to draw the arrows \n",
    "    \"\"\"\n",
    "    height, width = S.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "            \n",
    "            else:\n",
    "                # Adding the arrows to the plot\n",
    "                if 'up' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0, 0.3, head_width = 0.05, head_length = 0.05)\n",
    "                if 'down' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0, -0.3, head_width = 0.05, head_length = 0.05)\n",
    "                if 'left' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, -0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "                if 'right' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "\n",
    "\n",
    "    offset = .5    \n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "def plot_policy_value_matrix(\n",
    "    P: dict, \n",
    "    S: np.array, \n",
    "    V: np.array, \n",
    "    goal_coords: list = [], \n",
    "    img_width: int = 5, \n",
    "    img_height: int = 5, \n",
    "    title: str = None, \n",
    "    annotate_goal: bool = False\n",
    "    ) -> None: \n",
    "    \"\"\" \n",
    "    Plots the policy matrix out of the dictionary provided; The dictionary values are used to draw the arrows \n",
    "    \"\"\"\n",
    "    height, width = S.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    # The first plot is the value matrix \n",
    "    ax = fig.add_subplot(121, aspect='equal')\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # By default, the (0, 0) coordinate in matplotlib is the bottom left corner,\n",
    "            # so we need to invert the y coordinate to plot the matrix correctly\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "                if annotate_goal:\n",
    "                    ax.annotate(str(V[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "            else: \n",
    "                ax.annotate(str(V[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "\n",
    "    offset = .5\n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "    ax.set_title('Value Matrix')\n",
    "\n",
    "    # The second plot is the policy matrix \n",
    "    ax = fig.add_subplot(122, aspect='equal')\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "            \n",
    "            else:\n",
    "                # Adding the arrows to the plot\n",
    "                if 'up' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0, 0.3, head_width = 0.05, head_length = 0.05)\n",
    "                if 'down' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0, -0.3, head_width = 0.05, head_length = 0.05)\n",
    "                if 'left' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, -0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "                if 'right' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "    \n",
    "    offset = .5\n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "    ax.set_title('Policy Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ca8b6",
   "metadata": {},
   "source": [
    "## Action set\n",
    "\n",
    "The $\\mathbb{A}$ set contains all the possible actions that the agent can take. In this case, the agent can move in any of the four directions, so the action set is $\\mathbb{A} = \\{up, down, left, right\\}$ or $\\mathbb{A} = \\{\\uparrow,  \\downarrow, \\leftarrow, \\rightarrow\\}$\n",
    "\n",
    "##  Reward matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356d734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:38:41.974920Z",
     "start_time": "2023-09-26T14:38:41.691903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the number of blocks of a n x n grid \n",
    "n = 7\n",
    "\n",
    "# Defining the value for the hole and the goal\n",
    "goal = 10\n",
    "step = -1\n",
    "\n",
    "# Initiating an empty dataframe of size n x n\n",
    "G = np.ones((n,n))\n",
    "\n",
    "# Defining the coordinates of the goal\n",
    "goal_coords = [(0, n-1), (n-1, 0), (0, 0), (n-1, n-1)]\n",
    "#goal_coords = [(1, 2)]\n",
    "# Adding the goal values to the center and the corners\n",
    "for goal_coord in goal_coords:\n",
    "    G[goal_coord[0], goal_coord[1]] = goal\n",
    "\n",
    "# Every other step is -1\n",
    "G[G == 1] = step\n",
    "\n",
    "# Converting the G matrix to int \n",
    "G = G.astype(int)\n",
    "\n",
    "plot_matrix(G, goal_coords, title='Gridworld')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b6dd1",
   "metadata": {},
   "source": [
    "In the above diagram, the gridworld is represented as a `n` by `n` matrix. Each cell in it represent the reward you get when you transition to that state. We can call the above matrix as the `reward matrix` and denote it $\\mathbb{G}$. Each element in the matrix is a real number: \n",
    "\n",
    "$\\forall r \\in \\mathbb{G}, r \\in \\mathbb{R}$ \n",
    "\n",
    "## State matrix\n",
    "\n",
    "Alongside the $\\mathbb{G}$ matrix, we have the the state matrix $\\mathbb{S}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f336d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:38:44.546723Z",
     "start_time": "2023-09-26T14:38:44.254082Z"
    }
   },
   "outputs": [],
   "source": [
    "S = np.arange(0, n*n).reshape(n, n)\n",
    "\n",
    "plot_matrix(S, goal_coords, title='State space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2036592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:38:45.381172Z",
     "start_time": "2023-09-26T14:38:45.365520Z"
    }
   },
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c056e522",
   "metadata": {},
   "source": [
    "- The state matrix is just a matrix whose each element gives an index to the grid an agent is in. \n",
    "\n",
    "- For simplicity sake, we tend to flatten these matrices and not keep track of the row and the column indices - just the state numbers. \n",
    "- We can always go back to the plot above and check were a certain state is. \n",
    "\n",
    "## Policy matrix \n",
    "\n",
    "- The policy matrix, denoted as $\\mathbb{P}$ is a matrix whose each element is a probability of taking an action in a certain state. \n",
    "- In each of the elements of the grid, the values are an array of all the possible actions an agent can take.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f79b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:39:26.430003Z",
     "start_time": "2023-09-26T14:39:26.414004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving all the unique states to a vector \n",
    "states = np.unique(S)\n",
    "\n",
    "# Dictionary to hold each action for a given state\n",
    "P = {}\n",
    "for s in states: \n",
    "    s_dict = {}\n",
    "\n",
    "    # Checking which index is the current state in the S matrix \n",
    "    s_index = np.where(S == s)\n",
    "\n",
    "    # If the state is in the top left corner, we can only move right and down\n",
    "    if s_index == (0, 0):\n",
    "        s_dict['right'] = 0.5\n",
    "        s_dict['down'] = 0.5\n",
    "    \n",
    "    # If the state is in the top right corner, we can only move left and down\n",
    "    elif s_index == (0, n - 1):\n",
    "        s_dict['left'] = 0.5\n",
    "        s_dict['down'] = 0.5\n",
    "    \n",
    "    # If the state is in the bottom left corner, we can only move right and up\n",
    "    elif s_index == (n - 1, 0):\n",
    "        s_dict['right'] = 0.5\n",
    "        s_dict['up'] = 0.5\n",
    "    \n",
    "    # If the state is in the bottom right corner, we can only move left and up\n",
    "    elif s_index == (n - 1, n - 1):\n",
    "        s_dict['left'] = 0.5\n",
    "        s_dict['up'] = 0.5\n",
    "    \n",
    "    # If the state is in the first row, we can only move left, right, and down\n",
    "    elif s_index[0] == 0:\n",
    "        s_dict['left'] = 0.333\n",
    "        s_dict['right'] = 0.333\n",
    "        s_dict['down'] = 0.333\n",
    "    \n",
    "    # If the state is in the last row, we can only move left, right, and up\n",
    "    elif s_index[0] == n - 1:\n",
    "        s_dict['left'] =  0.333\n",
    "        s_dict['right'] = 0.333\n",
    "        s_dict['up'] = 0.333\n",
    "    \n",
    "    # If the state is in the first column, we can only move up, down, and right\n",
    "    elif s_index[1] == 0:\n",
    "        s_dict['up'] = 0.333\n",
    "        s_dict['down'] = 0.333\n",
    "        s_dict['right'] = 0.333\n",
    "    \n",
    "    # If the state is in the last column, we can only move up, down, and left\n",
    "    elif s_index[1] == n - 1:\n",
    "        s_dict['up'] = 0.333\n",
    "        s_dict['down'] = 0.333\n",
    "        s_dict['left'] = 0.333\n",
    "\n",
    "    # If the state is in the middle, we can move in all directions\n",
    "    else:\n",
    "        s_dict['up'] = 0.25\n",
    "        s_dict['down'] = 0.25\n",
    "        s_dict['left'] = 0.25\n",
    "        s_dict['right'] = 0.25\n",
    "\n",
    "    # Saving the current states trasition probabilities\n",
    "    P[s] = s_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ed88c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:39:28.043568Z",
     "start_time": "2023-09-26T14:39:27.781932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drawing a plot for the policy matrix with arrows; In one cell there can be the maximum of 4 arrows each indicating the action an agent can take \n",
    "plot_policy_matrix(P, S, goal_coords, title='Policy matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33be98",
   "metadata": {},
   "source": [
    "## Value matrix \n",
    "\n",
    "- The value matrix is denoted as $\\mathbb{V}$. \n",
    "- The dimension of the matrix is the same as the state space. \n",
    "- Each grid in the value matrix represents the total reward an agent can get if it starts from that state and follows the policy. \n",
    "\n",
    "$$v_{\\pi} (s) = \\mathbb{E}\\left[G_{t} | S_{t} = s \\right] $$\n",
    "\n",
    "Where \n",
    "\n",
    "$G_{t}$ - total reward an agent gets after taking action `a` in state `s` and following the policy $\\pi$ till the end of the episode. The equation for $G_{t}$ is given below.\n",
    "\n",
    "$$G_{t} = R_{t +1} + \\gamma R_{t + 2} + ... + \\gamma ^{K} R_{t+K}$$\n",
    " \n",
    "$K$ - the number of steps in the episode.\n",
    "\n",
    "$$v_{\\pi} (s) = \\sum_{a \\in \\mathbb{A}} \\left[ \\pi(a | s) \\sum_{s^{'}, r} p(s^{'}, r | s, a) \\left[ r + \\gamma v_{\\pi} (s^{'}) \\right] \\right] $$\n",
    "\n",
    "- This equation is called the Bellman equation.\n",
    "\n",
    "Where, \n",
    "\n",
    "$\\pi(a | s)$ is the probability of taking action `a` in state `s`\n",
    "\n",
    "$p(s^{'}, r | s, a)$ is the probability of transitioning to state `s'` with reward `r` when taking action `a` in state `s`\n",
    "\n",
    "$\\gamma \\in (0, 1)$ is the discount factor\n",
    "\n",
    "$v_{\\pi} (s)$ is the value of state `s` under policy $\\pi$. \n",
    "\n",
    "$r$ - reward for taking action `a` in state `s`. \n",
    "\n",
    "- The above equation is a recursive one and could go on forever. \n",
    "- In practice, we use a finite number of iterations to calculate the value of each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcada1a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:02.767447Z",
     "start_time": "2023-09-26T14:40:02.501579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiating the empty Value function \n",
    "V = np.zeros((n, n))\n",
    "\n",
    "plot_matrix(V, goal_coords, title='Value function', annotate_goal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c10b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:03.909935Z",
     "start_time": "2023-09-26T14:40:03.397276Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_policy_value_matrix(P, S, V, goal_coords, title='Policy value matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e8c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "123a72e0",
   "metadata": {},
   "source": [
    "# Example - Frozen Lake with Gym\n",
    "\n",
    "## Creating our first gym environment\n",
    "\n",
    "- Gym provides a variety of environments for training the reinforcement learning agent. \n",
    "\n",
    "- Let's introduce one of the simplest environments called the frozen lake environment. \n",
    "- The goal of the agent is to start from the initial state S and reach the goal state G.\n",
    "\n",
    "- In the frozen lake environment, the following applies:\n",
    "\n",
    "* S denotes the starting state\n",
    "* F denotes the frozen state\n",
    "* H denotes the hole state\n",
    "* G denotes the goal state\n",
    "\n",
    "- We need to make sure that the agent starts from S and reaches G without falling into the hole state H as shown below:\n",
    "\n",
    "- Each grid box in the environment is called state, thus we have 16 states (S to G) and we have 4 possible actions which are up, down, left and right. \n",
    "- We learned that our goal is to reach the state G from S without visiting H. So, we assign reward as 0 to all the states and + 1 for the goal state G. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e665292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:21.693716Z",
     "start_time": "2023-09-26T14:40:21.675876Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://user-images.githubusercontent.com/69793689/234189847-c582581a-2947-4b27-99e8-20b4a51221ed.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd476b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:22.479825Z",
     "start_time": "2023-09-26T14:40:22.461824Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can create a gym environment using the make function. \n",
    "# The make function requires the environment id as a parameter. \n",
    "# In the gym, the id of the frozen lake environment is FrozenLake-v0\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8cd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:23.521565Z",
     "start_time": "2023-09-26T14:40:23.165876Z"
    }
   },
   "outputs": [],
   "source": [
    "# see how our environment looks like using the render function\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f040ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:23.835930Z",
     "start_time": "2023-09-26T14:40:23.831960Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076adca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:24.601473Z",
     "start_time": "2023-09-26T14:40:24.597473Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac60b49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:25.243051Z",
     "start_time": "2023-09-26T14:40:25.238073Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d368457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:26.041413Z",
     "start_time": "2023-09-26T14:40:25.925633Z"
    }
   },
   "outputs": [],
   "source": [
    "# the environment will save all the frames internally, and you can retrieve all of them at once calling render\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array_list\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "frame_collection = env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4a997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:28.541242Z",
     "start_time": "2023-09-26T14:40:28.461649Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94a26f",
   "metadata": {},
   "source": [
    "## Transition probability and Reward function\n",
    "\n",
    "- We can obtain the transition probability and the reward function by just typing `env.P[state][action]` \n",
    "- To obtain the transition probability of moving from the state S to the other states by performing an action right, we can type, `env.P[S][right]`.\n",
    "- To obtain the transition probability of state S by performing an action right, we type `env.P[0][2]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3f7c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:35.975490Z",
     "start_time": "2023-09-26T14:40:35.962492Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.P[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7fa49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:36.523966Z",
     "start_time": "2023-09-26T14:40:36.507946Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.P[3][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd9f49",
   "metadata": {},
   "source": [
    "- What does this imply? Our output is in the form of `[(transition probability, next state, reward, Is terminal state?)]`\n",
    "- It implies that if we perform an action 2 (right) in state 0 (S) then:\n",
    "\n",
    "* We reach the state 4 (F) with probability 0.33333 and receive 0 reward. \n",
    "* We reach the state 1 (F) with probability 0.33333 and receive 0 reward.\n",
    "* We reach the same state 0 (S) with probability 0.33333 and receive 0 reward.\n",
    "\n",
    "- Thus, when we type `env.P[state][action]` we get the result in the form of `[(transition probability, next state, reward, Is terminal state?)]`. \n",
    "- The last value is the boolean and it implies that whether the next state is a terminal state, since 4, 1 and 0 are not the terminal states it is given as false. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d4b39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:40:38.535279Z",
     "start_time": "2023-09-26T14:40:38.526277Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v1', render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2117aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
